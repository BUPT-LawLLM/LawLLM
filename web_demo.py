import json
import torch
import streamlit as st

from evaluator import WenshuEvaluator, LawQAEvaluator
from retriever import Retriever


st.set_page_config(page_title="BUPT-LawLLM âš–ï¸")
sidebar_options = {"æ³•å¾‹å’¨è¯¢é—®ç­” ğŸ‘©ğŸ»â€ğŸ’¼": "feature_1", "æ³•å¾‹æ–‡ä¹¦è¡¥å…¨ ğŸ“ƒ": "feature_2", "ç›¸å…³æ³•æ¡æ£€ç´¢ ğŸ”": "feature_3"}
st.sidebar.markdown("## BUPT-LawLLM âš–ï¸")
st.sidebar.markdown("æˆ‘æ˜¯ BUPT æ™ºèƒ½ç³»ç»Ÿå®éªŒå®¤åŸºäº ChatGLM3 ç­‰å¼€æº LLM å¾®è°ƒå¹¶è®¾è®¡çš„æ³•å¾‹é¢†åŸŸ AI åŠ©æ‰‹ã€‚")
selected_option = st.sidebar.selectbox("", list(sidebar_options.keys()))

if 'case' not in st.session_state:
    st.session_state['case'] = ""
if 'judgeAccusation' not in st.session_state:
    st.session_state['judgeAccusation'] = ""
if 'judgeReason' not in st.session_state:
    st.session_state['judgeReason'] = ""


@st.cache_resource
def init_model():
    model = WenshuEvaluator(k=0, top_p=0.9, temperature=0.1, relevant_k=5, finetuned=True, source=None, verbose=True)
    retriever = Retriever(relevant_k=5, source="local") 
    return model, retriever

def clear_chat_history():
    if st.session_state.messages:
        del st.session_state.messages

def init_chat_history():
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    for message in st.session_state.messages:
        avatar = "ğŸ§‘ğŸ»â€ğŸ’»" if message["role"] == "user" else "âš–ï¸"
        with st.chat_message(message["role"], avatar=avatar):
            st.markdown(message["content"])

    return st.session_state.messages


def main():
    model, retriever = init_model()
    
    with st.chat_message("assistant", avatar="âš–ï¸"):
        st.markdown(f"æ‚¨å¥½ï¼Œæˆ‘æ˜¯ BUPT-LawLLMï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ã€‚æ‚¨å¯ä»¥åœ¨å·¦ä¾§è¾¹æ åˆ‡æ¢æˆ‘çš„åŠŸèƒ½æ¨¡å¼ã€‚å¦‚æœæˆ‘çš„ç”Ÿæˆæœ‰è¯¯ï¼Œè¯·ä»¥ä¸“ä¸šå¾‹å¸ˆå’Œå›½å®¶å‡ºå°çš„æ³•æ¡ä¸ºåŸºå‡†ã€‚") 

    if sidebar_options[selected_option] == "feature_1":
        messages = init_chat_history()

        if prompt := st.chat_input("Shift + Enter æ¢è¡Œï¼ŒEnter å‘é€"):
            with st.chat_message("user", avatar="ğŸ§‘ğŸ»â€ğŸ’»"):
                st.markdown(prompt)

            messages.append({"role": "user", "content": prompt})
            print(f"[user] {prompt}", flush=True)
            with st.chat_message("assistant", avatar="ğŸ‘©ğŸ»â€ğŸ’¼"):
                placeholder = st.empty()
                responses = [model.model_generate("ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šå¾‹å¸ˆï¼Œå¦‚ä¸‹æ˜¯ä¸€ä¸ªæ³•å¾‹å’¨è¯¢é—®é¢˜ï¼š"+message["content"]+"\nè¯·æ ¹æ®ç›¸å…³æ³•æ¡å’Œä½ çš„çŸ¥è¯†è¿›è¡Œå›ç­”ã€‚ä½ çš„å›ç­”ï¼š") for message in messages]
                for response in responses:
                    placeholder.markdown(response)
                    if torch.backends.mps.is_available():
                        torch.mps.empty_cache()

            messages.append({"role": "assistant", "content": response})
            print(json.dumps(messages, ensure_ascii=False), flush=True)
            st.button("æ¸…ç©ºå¯¹è¯", on_click=clear_chat_history)

    elif sidebar_options[selected_option] == "feature_2":
        messages = init_chat_history()

        # if not st.session_state.case and not st.session_state.judgeAccusation and not st.session_state.judgeReason:
        #     with st.chat_message("assistant", avatar="ğŸ“ƒ"):
        #         st.markdown("æ–‡ä¹¦çš„æ¡ˆä»¶åç§°æ˜¯ä»€ä¹ˆï¼Ÿ")

        if step := st.chat_input(f"Shift + Enter æ¢è¡Œï¼ŒEnter å‘é€"):
            if (not st.session_state.case) and (not st.session_state.judgeAccusation) and (not st.session_state.judgeReason):
                st.session_state.case = step
                with st.chat_message("user", avatar="ğŸ§‘ğŸ»â€ğŸ’»"):
                    st.markdown("å·²æä¾›æ¡ˆä»¶åç§°ã€‚")
                with st.chat_message("assistant", avatar="ğŸ“ƒ"):
                    st.markdown("å¥½çš„ï¼Œæ¡ˆä»¶çš„æŒ‡æ§éƒ¨åˆ†è¯´äº†ä»€ä¹ˆï¼Ÿ")

            elif (st.session_state.case) and (not st.session_state.judgeAccusation) and (not st.session_state.judgeReason):
                st.session_state.judgeAccusation = step
                with st.chat_message("user", avatar="ğŸ§‘ğŸ»â€ğŸ’»"):
                    st.markdown("å·²æä¾›æŒ‡æ§éƒ¨åˆ†ã€‚")
                with st.chat_message("assistant", avatar="ğŸ“ƒ"):
                    st.markdown("å¥½çš„ï¼Œé‚£ä¹ˆæ³•é™¢ç»™å‡ºçš„è¿‡ç¨‹æ¨ç†æ˜¯æ€æ ·çš„ï¼Ÿ")
                # messages.append({"role": "assistant", "content": "å¥½çš„ï¼Œé‚£ä¹ˆæ³•é™¢ç»™å‡ºçš„è¿‡ç¨‹æ¨ç†æ˜¯æ€æ ·çš„ï¼Ÿ"})

            elif (st.session_state.case) and (st.session_state.judgeAccusation) and (not st.session_state.judgeReason):
                st.session_state.judgeReason = step
                prompt = {"Case": st.session_state.case, "JudgeAccusation": st.session_state.judgeAccusation, "JudgeReason": st.session_state.judgeReason}
                messages.append({"role": "user", "content": prompt})
                print(f"[user] {prompt}", flush=True)

                with st.chat_message("assistant", avatar="ğŸ“ƒ"):
                    placeholder = st.empty()
                    responses = [model.run(message["content"]) for message in messages]
                    for response in responses:
                        placeholder.markdown(response)
                        if torch.backends.mps.is_available():
                            torch.mps.empty_cache()

                messages.append({"role": "assistant", "content": response})
                print(json.dumps(messages, ensure_ascii=False), flush=True)
                st.session_state.case, st.session_state.judgeAccusation, st.session_state.judgeReason = "", "", ""
                st.button("æ¸…ç©ºå¯¹è¯", on_click=clear_chat_history)

    elif sidebar_options[selected_option] == "feature_3":
        messages = init_chat_history()

        if prompt := st.chat_input("Shift + Enter æ¢è¡Œï¼ŒEnter å‘é€"):
            with st.chat_message("user", avatar="ğŸ§‘ğŸ»â€ğŸ’»"):
                st.markdown(prompt)

            messages.append({"role": "user", "content": prompt})
            print(f"[user] {prompt}", flush=True)
            with st.chat_message("assistant", avatar="ğŸ”"):
                placeholder = st.empty()
                outputs = []
                for message in messages:
                    output = retriever.query(prompt)
                    outputs.append(output)
                
                for response in outputs:
                    placeholder.markdown(response)
                    if torch.backends.mps.is_available():
                        torch.mps.empty_cache()

            messages.append({"role": "assistant", "content": response})
            print(json.dumps(messages, ensure_ascii=False), flush=True)
            st.button("æ¸…ç©ºå¯¹è¯", on_click=clear_chat_history)


if __name__ == "__main__":
    main()
